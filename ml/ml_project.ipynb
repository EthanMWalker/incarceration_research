{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg as la\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import pickle\n",
    "import time\n",
    "import xgboost\n",
    "\n",
    "from sklearn import linear_model, model_selection, metrics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as GDA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score\n",
    "from sklearn.metrics import roc_auc_score as RAS\n",
    "from sklearn.metrics import roc_curve as ROC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jupyter nbconvert --to latex \"Race and Incarceration in America.ipynb\" --TagRemovePreprocessor.remove_input_tags='{\"hide_input\"}'; pdflatex \"Race and Incarceration in America\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Machine Learning is becomeing a more prevelant tool in the world of criminal justice. Often it is used to predict who will commit a crime or where crimes may occur. Seldom is it used to regulate the criminal justice system.\n",
    "\n",
    "# Problem Statement and Motivation\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Last semester I took a look a data set containing the information of more than 7.5 million individuals that have been processed by the criminal justice system. I found that racial minorities were more likely to receive extreme sentences, agreeing with existing research around bias in the criminal justice system. In this project I will be exploring the data from a machine learning perspective. My goal is to determine if this data can be classified in such a way that is predictive of race. The idea is that perhaps racial bias can be detected in various systems by seeing how effective different machine learning techniques are at classifying inmate data by race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Source and Credibility\n",
    "The data that I will be using in this analysis is from one source. It is a [database](https://catalog.data.gov/dataset/sentenced-inmates-in-correctional-facilities) hosted on [Data.gov](https://www.data.gov) and maintained by the State of Connecticut Department of Corrections. This source is highly credible because it is a primary source for the data. This organization is an official government agency which collects, maintains, and reports on this data.\n",
    "\n",
    "## Gathering and Cleaning\n",
    "All the data which I am using in this report are freely available to the public. Collection and cleaning was relatively simple as the source data was well maintained. The file that I obtained from the Connecticut Department of Corrections is a very well maintained database. The largest issue I had with this file was mild inconsistency with the way in which certain data was encoded (ex. race was encoded as both `WHITE` and `WHITE\\t`). This was the data that I spent the most time working to engineer as it is a data set that I intend to use for different regression-related analyses. The files are\n",
    "```\n",
    "individuals.csv\n",
    "regression_df.csv.\n",
    "\n",
    "```\n",
    "## About the Data\n",
    "This data set contains individual information for 7.77 million people that have been processed by the justice system and recorded by the Connecticut Department of Corrections. Each individual is recorded along with their age, gender, race, offense, and sentence length, among other things [6].\n",
    "\n",
    "I also created one-hot encoded versions of this data set in order to run regressions on the data. Because of the size of the data, the regression data sets are only random subsets of the larger data set.\n",
    "\n",
    "Because there is so much to consider in what is found in the data set, I chose not to engineer more features as to avoid unneeded complexity.\n",
    "\n",
    "i ought to disclose the sample sizes among different races that are found in the Connecticut Department of Justice data. The sample size for American Indians and Asians is much smaller than that of Whites, Hispanics, and Blacks, hence we may see some irregular outcomes in the analysis related to these racial groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['RACE','GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']\n",
    "features = ['GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']\n",
    "df = pd.read_csv('individuals.csv',usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for Blacks: 3287596\n",
      "Sample size for Whites: 2393949\n",
      "Sample size for Hispanic: 2039297\n",
      "Sample size for American Indian: 21133\n",
      "Sample size for Asian: 35660\n"
     ]
    }
   ],
   "source": [
    "races = {'BLACK':'Blacks','WHITE':'Whites',\n",
    "         'HISPANIC':'Hispanic','AMER IND':'American Indian',\n",
    "         'ASIAN':'Asian'\n",
    "        }\n",
    "for rac in races.keys():\n",
    "    mask = df.RACE == rac\n",
    "    print(f'Sample size for {races[rac]}: {len(df[mask])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible questions\n",
    "can we use ml techniques to correctly classify this data?\n",
    "which ones fail and why?\n",
    "can we create a predictive model for sentence lengths?\n",
    "should sentencing be offloaded to a ml algorithm?\n",
    "what does it mean to have an effective classfifier for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunksize = 100000\n",
    "# rdf = pd.read_csv('regression_df.csv', chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['RACE','GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']\n",
    "features = ['GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']\n",
    "df = pd.read_csv('individuals.csv',usecols=cols) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Techniques\n",
    "TSNE is having poor results and UMAP doesn't seem to capture any good groupings.\n",
    "### TALK ABOUT UMAP MORE\n",
    "I could block by crime and examine a few \"big\" crimes the try several different perplexities, however I do not think that clusting will be helpful in this case. Especially becasue I have not yet found a kernel that converts crimes to a spacial variable.\n",
    "\n",
    "there are other techniques that are not applicable to this data. For example, since this data set is not a time series models like ARMA and HMM are not applicable here.\n",
    "\n",
    "## Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(20000)\n",
    "samp.RACE = pd.factorize(samp['RACE'])[0] + 1\n",
    "samp.GENDER = pd.factorize(samp['GENDER'])[0] + 1\n",
    "samp.OFFENSE = pd.factorize(samp['OFFENSE'])[0] + 1\n",
    "samp.DETAINER = pd.factorize(samp['DETAINER'])[0] + 1\n",
    "samp.FACILITY = pd.factorize(samp['FACILITY'])[0] + 1\n",
    "samp_y = samp.RACE\n",
    "samp_X = samp[['GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to train is 223.73655876318614 minutes\n",
      "oob score is 0.70645\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': np.arange(100,400,20),\n",
    "    'max_depth': np.arange(10,100,10),\n",
    "    'max_features': np.arange(1,6)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(oob_score=True)\n",
    "\n",
    "s = time.time()\n",
    "clf = GridSearchCV(clf, param_grid, scoring=None, cv=5)\n",
    "clf = clf.fit(samp_X, samp_y)\n",
    "e = time.time()\n",
    "print(f'time to train is {(e-s)/60} minutes')\n",
    "\n",
    "clf = clf.best_estimator_\n",
    "clf = clf.fit(samp_X, samp_y)\n",
    "print(f'oob score is {clf.oob_score_}')\n",
    "\n",
    "with open('RandomForestClf.pickle', \"wb+\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/.local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.3 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70645\n",
      "[0.01022266 0.26942225 0.2124008  0.17300606 0.0338177  0.30113052]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/.local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.3 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('RandomForestClf.pickle',\"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "print(clf.oob_score_)\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can see that the most important features are sentence length, age, and offense. And the high OoB score is promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.420315"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp = df.sample(200000)\n",
    "samp.RACE = pd.factorize(samp['RACE'])[0] + 1\n",
    "samp.GENDER = pd.factorize(samp['GENDER'])[0] + 1\n",
    "samp.OFFENSE = pd.factorize(samp['OFFENSE'])[0] + 1\n",
    "samp.DETAINER = pd.factorize(samp['DETAINER'])[0] + 1\n",
    "samp.FACILITY = pd.factorize(samp['FACILITY'])[0] + 1\n",
    "samp_y = samp.RACE\n",
    "samp_X = samp[['GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']]\n",
    "clf.score(samp_X, samp_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the score is not great. Random chance would be .2, so it does do better than chance, though not much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Boosted Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(10000)\n",
    "samp.RACE = pd.factorize(samp['RACE'])[0] + 1\n",
    "samp.GENDER = pd.factorize(samp['GENDER'])[0] + 1\n",
    "samp.OFFENSE = pd.factorize(samp['OFFENSE'])[0] + 1\n",
    "samp.DETAINER = pd.factorize(samp['DETAINER'])[0] + 1\n",
    "samp.FACILITY = pd.factorize(samp['FACILITY'])[0] + 1\n",
    "samp_y = samp.RACE\n",
    "samp_X = samp[['GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': np.linspace(.01,1,10),\n",
    "    'subsample': np.linspace(.05,1,10),\n",
    "    'max_depth': np.arange(1,5)\n",
    "}\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=5000)\n",
    "\n",
    "s = time.time()\n",
    "# clf = GridSearchCV(clf, param_grid, cv=5)\n",
    "clf = clf.fit(samp_X, samp_y)\n",
    "e = time.time()\n",
    "\n",
    "# clf = clf.best_estimator_\n",
    "# clf = clf.fit(samp_X, samp_y)\n",
    "\n",
    "print(f'time is {(e-s)/60}')\n",
    "\n",
    "with open('GradientBoostedClf1.pickle', \"wb+\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.296365"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp = df.sample(200000)\n",
    "samp.RACE = pd.factorize(samp['RACE'])[0] + 1\n",
    "samp.GENDER = pd.factorize(samp['GENDER'])[0] + 1\n",
    "samp.OFFENSE = pd.factorize(samp['OFFENSE'])[0] + 1\n",
    "samp.DETAINER = pd.factorize(samp['DETAINER'])[0] + 1\n",
    "samp.FACILITY = pd.factorize(samp['FACILITY'])[0] + 1\n",
    "samp_y = samp.RACE\n",
    "samp_X = samp[['GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']]\n",
    "clf.score(samp_X, samp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02090027 0.24716962 0.25775903 0.10433119 0.03748648 0.33235341]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/.local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.20.3 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ethan/.local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator GradientBoostingClassifier from version 0.20.3 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('GradientBoostedClf.pickle', \"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "#     print(f'oob improvement {clf.oob_improvement_}')\n",
    "#     to_print = zip(clf.feature_importances_,features)\n",
    "    print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the same feature importances as we did with the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.34,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 4,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 600,\n",
       " 'n_iter_no_change': None,\n",
       " 'presort': 'auto',\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(10000)\n",
    "samp.RACE = pd.factorize(samp['RACE'])[0] + 1\n",
    "samp.GENDER = pd.factorize(samp['GENDER'])[0] + 1\n",
    "samp.OFFENSE = pd.factorize(samp['OFFENSE'])[0] + 1\n",
    "samp.DETAINER = pd.factorize(samp['DETAINER'])[0] + 1\n",
    "samp.FACILITY = pd.factorize(samp['FACILITY'])[0] + 1\n",
    "samp_y = samp.RACE\n",
    "samp_X = samp[['GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time was 1.7242353409528732 hours\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'learning_rate':np.linspace(.01,1,5),\n",
    "    'reg_alpha':np.linspace(.01,1,5),\n",
    "    'reg_lambda':np.linspace(.01,1,5),\n",
    "    'gamma':np.linspace(.01,1,5)\n",
    "}\n",
    "\n",
    "clf = xgboost.XGBClassifier(verbose=2)\n",
    "s = time.time()\n",
    "clf = GridSearchCV(clf, param_grid,cv=5)\n",
    "clf = clf.fit(samp_X, samp_y)\n",
    "\n",
    "with open('XBG_fitted_grid.pickle', 'wb+') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "clf = clf.best_estimator_\n",
    "clf = clf.fit(samp_X, samp_y)\n",
    "e = time.time()\n",
    "\n",
    "print(f'time was {(e-s)/(60*60)} hours')\n",
    "\n",
    "with open('XGBoostClf.pickle', 'wb+') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(200000)\n",
    "samp.RACE = pd.factorize(samp['RACE'])[0] + 1\n",
    "samp.GENDER = pd.factorize(samp['GENDER'])[0] + 1\n",
    "samp.OFFENSE = pd.factorize(samp['OFFENSE'])[0] + 1\n",
    "samp.DETAINER = pd.factorize(samp['DETAINER'])[0] + 1\n",
    "samp.FACILITY = pd.factorize(samp['FACILITY'])[0] + 1\n",
    "samp_y = samp.RACE\n",
    "samp_X = samp[['GENDER','AGE','OFFENSE','FACILITY','DETAINER','SENTENCE DAYS']]\n",
    "# clf.score(samp_X,samp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41374\n",
      "[0.2596316  0.132617   0.1624777  0.12035192 0.18768501 0.13723671]\n"
     ]
    }
   ],
   "source": [
    "with open('XGBoostClf.pickle', \"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "#     print(f'oob improvement {clf.oob_improvement_}')\n",
    "#     to_print = zip(clf.feature_importances_,features)\n",
    "    print(clf.score(samp_X,samp_y))\n",
    "    print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score is very promiseing since the score is about double chance. There actually is a lot of correct classification going on here. \n",
    "\n",
    "Interestingly however, it seems that the feature importances are very different for this model. Gender is the most important feature nad every other feature is about equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KD Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "# Analysis\n",
    "\n",
    "\n",
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
